{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL(reinforcement learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -1. Reference\n",
    "[sutton](http://incompleteideas.net/sutton/book/bookdraft2016sep.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 Basic\n",
    "매 time step마다  \n",
    "**Agent** : state $S_t$에 대해서 각각의 policy $\\pi$에 따라서 action $A_t$를 수행  \n",
    ">  단, policy $\\pi$ can be deterministic or stochastic over $S_t$\n",
    ">  또한, policy 는 간단하게 *table*로 주어 질 수도 있고,   \n",
    "state 들이 input이고 action들이 output인 *black box function* 형태로 주어질 수 있다.(ex. Neural Net)  \n",
    "\n",
    "**Environment** : Agent의 action $A_t$를 받고 다음 state $S_{t+1}$를 주고 Reward  $R_{t+1}$을 준다.   \n",
    "&nbsp;&nbsp;&nbsp; *state(S), action(A), reward(R)*\n",
    "> Reward 는 $\\mathscr{S}\\times \\mathscr{A}$에서 $\\mathbb{R}$ 로 가는 함수로 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Objective\n",
    "**Agent** 의 목적은 앞으로 받을 reward의 기댓값을 최대화 하도록 동작한다. \n",
    ">$G_t = R_{t+1}+R_{t+2}+\\cdots + R_T$(단, $T$ 는 final step)  \n",
    ">으로 정할 수 도 있지만, 이렇게 하면,   \n",
    ">$T= \\infty$ 이면 발산하게 되서 문제가 많이 생길 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma$ : discount rate ($0<\\gamma<1$)\n",
    ">$G_t = R_{t+1}+\\gamma R_{t+2}+\\cdots + = \\sum_{k=0}^{\\infty} \\gamma^{k}R_{t+k+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 MDP(markov decision proceses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov property를 만족하는 RL의 과정을 MDP라고 한다.  \n",
    "state, action이 유한하면 finite MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Pr(S_{t+1}=s', R_{t+1}=r|S_0,A_0,R_1,\\cdots, S_{t-1},A_{t-1},R_t,S_t,A_t)$$\n",
    "$$\\Rightarrow Pr(S_{t+1}=s', R_{t+1}=r|S_t=s,A_t=a)$$\n",
    "$$\\Rightarrow p(s',r|s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$p(s',r|s,a)$ 이걸로 다른 것들을 표현 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex)\n",
    "$$r(s,a) =E[R_{t+1}|S_t=s, A_t=a]=\\sum_{r\\in \\mathscr{R}} r \\sum_{s'\\in \\mathscr{S}}p(s',r|s,a)$$\n",
    "$$p(s'|s,a) =\\sum_{r\\in \\mathscr{R}} p(s',r|s,a)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{\\pi}(s) = E_{\\pi}[G_t|S_t=s]\n",
    "=E_{\\pi}[R_{t+1}+\\sum_{k=1}^{\\infty} \\gamma^{k}R_{t+k+1}|S_t=s]$$\n",
    "\n",
    "$$= \\sum_{a} \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r+\\gamma E_{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^{k}R_{t+k+2}|S_{t+1}=s']]$$\n",
    "\n",
    "$$= \\sum_{a} \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r+\\gamma E_{\\pi}[G_{t+1}|S_{t+1}=s']]$$\n",
    "\n",
    "$$= \\sum_{a} \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so called **Bellman Equation**  \n",
    "물론 이 것을 각각의 state 의 Value function $v_{\\pi}(s)$를 변수라고 생각하고 연립방정식을 풀 수 있다. 각 state 마다 식이 하나 씩 나오므로 식과 변수의 갯수가 같으므로 풀 수 있다. 하지만, 풀지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "$v_{\\pi}(s)$ 는 policy $\\pi$에 dependent 하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{\\pi}(s,a)=E_{\\pi}[G_t|S_t=s, A_t=a]$$\n",
    "$$=E_{\\pi}[R_{t+1}+\\sum_{k=1}^{\\infty} \\gamma^{k}R_{t+k+1}|S_t=s, A_t =a]$$\n",
    "\n",
    "$$= \\sum_{s',r}p(s',r|s,a)\\big{(}r+\\gamma E_{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^{k}R_{t+k+2}|S_t=s']\\big{)}$$\n",
    "\n",
    "$$= \\sum_{s',r}p(s',r|s,a)\\big{(}r+\\gamma v_{\\pi}(s')\\big{)}$$\n",
    "$$= \\sum_{s',r}p(s',r|s,a)\\big{(}r+\\gamma \\sum_{a'} \\pi(a'|s')E_{\\pi}[G_{t+1}|S_t=s', A_t = a']\\big{)}$$\n",
    "\n",
    "$$= \\sum_{s',r}p(s',r|s,a)[r+\\gamma \\sum_{a'} \\pi(a'|s')q_{\\pi}(s',a')]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Optimal value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{*}=\\max_{\\pi} v_\\pi(s)$$\n",
    "$$q_{*}=\\max_{\\pi} q_{\\pi}(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{*}= \\max_{a\\in \\mathscr{A}} \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이론적으로는 괜찮지만, 많은 계산량 때문에 잘 쓰지 않는 방법\n",
    "* Policy iteration and Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**policy evaluation** 과 **policy improvement**의 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정해진 policy $\\pi$에 대해서 각각의 value function 구하는 방법, 연립 방정식을 풀어도 되지만, 다른 방법을 제안한다.\n",
    "\n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')]$$\n",
    "\n",
    "$v_{\\pi}$로 수렴하는 value function의 수열 $v_0 \\rightarrow v_1 \\rightarrow v_2 \\rightarrow \\cdots v_{\\pi}$을 구한다.  \n",
    "모든 state $s$에서 \n",
    "$$v_{k+1}(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_k(s')]$$\n",
    "로 update 한다. 해당 값이 수렴할 때 까지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "<img src = './image/policy_evaluation.png'>\n",
    "\n",
    "처음 시작한 policy $\\pi$는 각각의 state로 부터 모든 방향에 동일한 확률로 움직이게 된다. 왼쪽 그림은 매 update step마다 각각의 state에 대한 value_functiond 값을 나타내고, 오른쪽 그림은 각각의 state에서 value function의 값이 높은 곳을 가르킨다. 즉 가장 이상적인 action의 값을 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Value function to action-value function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{\\pi}(s,a)= \\sum_{s',r}p(s',r|s,a)\\big{(}r+\\gamma v_{\\pi}(s')\\big{)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Action-value function to policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi'(s) = \\arg \\max_{a} q_\\pi(s,a)$$\n",
    "$$= \\arg \\max_{a}\\sum_{s',r}p(s',r|s,a)\\big{(}r+\\gamma v_{\\pi}(s')\\big{)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최댓값이 두 개 이상이면 stochastic 한 policy로 생성된다.  \n",
    "하나면 deterministic 한 policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면, 이 새로운 policy는\n",
    "$$v_{\\pi}(s) \\leq v_{\\pi'}(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Final remark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    더 이상 policy improvement가 필요없어질 때까지 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Policy iteration이 너무 느리다는 단점을 극복하기 때문에 나왔다. \n",
    "* Policy iteration에서 policy evaluation을 수렴할 때 까지 돌리지 않고 policy evaluation을 한 step, policy improvement을 한 step 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_{*}$로 수렴하는 value function의 수열 $v_0 \\rightarrow v_1 \\rightarrow v_2 \\rightarrow \\cdots $을 구한다.\n",
    "\n",
    "$$v_{k+1}(s) = \\max_a \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{k}(s')]$$\n",
    "  \n",
    ">cf)\n",
    "$$v_{k+1}(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_k(s')]$$\n",
    "\n",
    "로 update 한다. 해당 값이 수렴할 때 까지 반복하고,  \n",
    "**최종 policy**  \n",
    "$$\\pi(x)=\\arg\\max_a\\sum_{s',r} p(s',r|s,a)[r+ \\gamma V(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MC method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Environment에 대한 prior knowledge 없이 real experience를 바탕으로 return $G_t$를 구하고 그것을 바탕으로 학습된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "많이 쓰진 않는 방법은 아니므로 간단한 MC style algorithm 하나만 살펴보고 넘어가도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src = './image/MC.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$Q(s, a) : (nstate space)\\times (naction space at $s$) \\rightarrow \\mathbb{R}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "greedy policy : $\\pi(a|s) = \\arg\\max_{a}Q(s,a) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon$-greedy policy : $\\pi(a|s)$는  $1-\\epsilon$확률로 $\\arg\\max_{a}Q(s,a)$ 를 선택하고 $\\epsilon$확률로 action을 random으로 고른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 보통 처음에는 $\\epsilon$ 을 1에 가깝게 잡고, 시간이 지남에 따라 감소시킨다.  \n",
    "> 처음에는 성능이 안좋을 수도 있는데, 길게 보면 결국 좋아진다.  \n",
    "> exploration and exploitation tradeoff!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC 에서 value function $V_(s)$를 update 시킬 수도 있고, $Q(s,a)$를 없데이트 시킬 수 있다.   \n",
    "$$V_(S_t) \\leftarrow V_(S_t)+\\alpha[G_t-V_(S_t)]$$\n",
    "or  \n",
    "$$Q(S_t, A_t) \\leftarrow V_(S_t)+\\alpha[G_t-Q(S_t, A_t)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TD method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V_(S_t) \\leftarrow V_(S_t)+\\alpha[R_{t+1}+\\gamma V_(S_{t+1})-V_(S_t)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = './image/TD.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q network\n",
    "## Policy gradient\n",
    "## DA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

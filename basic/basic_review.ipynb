{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL(reinforcement learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -1. Reference\n",
    "[sutton](http://incompleteideas.net/sutton/book/bookdraft2016sep.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 Basic\n",
    "매 time step마다  \n",
    "**Agent** : state $S_t$에 대해서 각각의 policy $\\pi$에 따라서 action $A_t$를 수행  \n",
    ">  단, policy $\\pi$ can be deterministic or stochastic over $S_t$\n",
    ">  또한, policy 는 간단하게 *table*로 주어 질 수도 있고,   \n",
    "state 들이 input이고 action들이 output인 *black box function* 형태로 주어질 수 있다.(ex. Neural Net)  \n",
    "\n",
    "**Environment** : Agent의 action $A_t$를 받고 다음 state $S_{t+1}$를 주고 Reward  $R_{t+1}$을 준다.   \n",
    "&nbsp;&nbsp;&nbsp; *state(S), action(A), reward(R)*\n",
    "> Reward 는 $\\mathscr{S}\\times \\mathscr{A}$에서 $\\mathbb{R}$ 로 가는 함수로 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Objective\n",
    "**Agent** 의 목적은 앞으로 받을 reward의 기댓값을 최대화 하도록 동작한다. \n",
    ">$G_t = R_{t+1}+R_{t+2}+\\cdots + R_T$(단, $T$ 는 final step)  \n",
    ">으로 정할 수 도 있지만, 이렇게 하면,   \n",
    ">$T= \\infty$ 이면 발산하게 되서 문제가 많이 생길 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma$ : discount rate ($0<\\gamma<1$)\n",
    ">$G_t = R_{t+1}+\\gamma R_{t+2}+\\cdots + = \\sum_{k=0}^{\\infty} \\gamma^{k}R_{t+k+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 MDP(markov decision proceses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov property를 만족하는 RL의 과정을 MDP라고 한다.  \n",
    "state, action이 유한하면 finite MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Pr(S_{t+1}=s', R_{t+1}=r|S_0,A_0,R_1,\\cdots, S_{t-1},A_{t-1},R_t,S_t,A_t)$$\n",
    "$$\\Rightarrow Pr(S_{t+1}=s', R_{t+1}=r|S_t=s,A_t=a)$$\n",
    "$$\\Rightarrow p(s',r|s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$p(s',r|s,a)$ 이걸로 다른 것들을 표현 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex)\n",
    "$$r(s,a) =E[R_{t+1}|S_t=s, A_t=a]=\\sum_{r\\in \\mathscr{R}} r \\sum_{s'\\in \\mathscr{S}}p(s',r|s,a)$$\n",
    "$$p(s'|s,a) =\\sum_{r\\in \\mathscr{R}} p(s',r|s,a)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{\\pi}(s) = E_{\\pi}[G_t|S_t=s]\n",
    "=E_{\\pi}[R_{t+1}+\\sum_{k=1}^{\\infty} \\gamma^{k}R_{t+k+1}|S_t=s]$$\n",
    "\n",
    "$$= \\sum_{a} \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r+\\gamma E_{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^{k}R_{t+k+2}|S_{t+1}=s']]$$\n",
    "\n",
    "$$= \\sum_{a} \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r+\\gamma E_{\\pi}[G_{t+1}|S_{t+1}=s']]$$\n",
    "\n",
    "$$= \\sum_{a} \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so called **Bellman Equation**  \n",
    "물론 이 것을 각각의 state 의 Value function $v_{\\pi}(s)$를 변수라고 생각하고 연립방정식을 풀 수 있다. 각 state 마다 식이 하나 씩 나오므로 식과 변수의 갯수가 같으므로 풀 수 있다. 하지만, 풀지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "$v_{\\pi}(s)$ 는 policy $\\pi$에 dependent 하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$q_{\\pi}(s,a)=E_{\\pi}[G_t|S_t=s, A_t=a]$$\n",
    "$$=E_{\\pi}[R_{t+1}+\\sum_{k=1}^{\\infty} \\gamma^{k}R_{t+k+1}|S_t=s, A_t =a]$$\n",
    "\n",
    "$$= \\sum_{s',r}p(s',r|s,a)\\big{(}r+\\gamma E_{\\pi}[\\sum_{k=0}^{\\infty} \\gamma^{k}R_{t+k+2}|S_t=s']\\big{)}$$\n",
    "$$= \\sum_{s',r}p(s',r|s,a)\\big{(}r+\\gamma \\sum_{a'} \\pi(a'|s')E_{\\pi}[G_{t+1}|S_t=s', A_t = a']\\big{)}$$\n",
    "\n",
    "$$= \\sum_{s',r}p(s',r|s,a)[r+\\gamma \\sum_{a'} \\pi(a'|s')q_{\\pi}(s',a')]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Optimal value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{*}=\\max_{\\pi} v_\\pi(s)$$\n",
    "$$q_{*}=\\max_{\\pi} q_{\\pi}(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{*}= \\max_{a\\in \\mathscr{A}} \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이론적으로는 괜찮지만, 많은 계산량 때문에 잘 쓰지 않는 방법\n",
    "* Policy iteration and Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**policy evaluation** 과 **policy improvement**의 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정해진 policy $\\pi$에 대해서 각각의 value function 구하는 방법, 연립 방정식을 풀어도 되지만, 다른 방법을 제안한다.\n",
    "\n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')]$$\n",
    "\n",
    "$v_{\\pi}$로 수렴하는 value function의 수열 $v_0 \\rightarrow v_1 \\rightarrow v_2 \\rightarrow \\cdots v_{\\pi}$을 구한다.  \n",
    "모든 state $s$에서 \n",
    "$$v_{k+1}(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_k(s')]$$\n",
    "로 update 한다. 해당 값이 수렴할 때 까지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "<img src = './image/policy_evaluation.png'>\n",
    "\n",
    "처음 시작한 policy $\\pi$는 각각의 state로 부터 모든 방향에 동일한 확률로 움직이게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MC method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TD method"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
